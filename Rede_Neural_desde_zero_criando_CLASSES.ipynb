{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede_Neural_desde_zero_criando_CLASSES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1qH270Ili9W26ZP688D5q5U9oEVfo-vP5",
      "authorship_tag": "ABX9TyPgYYRuJ0/0jb4/1A7D8/RI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EddyGiusepe/Neural_Network_from_scratch_only_with_Python/blob/main/Rede_Neural_desde_zero_criando_CLASSES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\">Rede Neural desde zero estilo Framework de Deep Learning</h2>\n",
        "\n",
        "\n",
        "\n",
        "Data Scientist.: Dr.Eddy Giusepe Chirinos Isidro"
      ],
      "metadata": {
        "id": "IraESWVRWH_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este estudo está baseado nos tutoriais de [Pepe Cantoral, PhD](https://www.youtube.com/watch?v=_shpKyA89QQ&list=PLWzLQn_hxe6ZlC9-YMt3nN0Eo-ZpOJuXd&index=18).\n",
        "\n",
        "Aqui estudaremos é implementaremos uma `REDE NEURAL Multi-capa` desde zero usando apenas `Python` e `NumPy`, sem usar Frameworks de Deep Learning como Tensorflow ou Pytorch."
      ],
      "metadata": {
        "id": "veLMHpb0XkrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rede Neural Multi-capa || OOP model"
      ],
      "metadata": {
        "id": "3sVqjjSTY76u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T0FWj35ZWGO6"
      },
      "outputs": [],
      "source": [
        "# Importamos nossas bibliotecas\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos as nossas Imagens"
      ],
      "metadata": {
        "id": "EkLHqi7-ZwZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar MNIST\n",
        "# Ver vídeo para aprender a importar estes Dados: https://www.youtube.com/watch?v=7cMKAlnSmpM\n",
        "\n",
        "import gzip\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "def list_files(mnist_path):\n",
        "    return [join(mnist_path,f) for f in os.listdir(mnist_path) if isfile(join(mnist_path, f))]\n",
        "\n",
        "def get_images(mnist_path):\n",
        "    for f in list_files(mnist_path):\n",
        "        if 'train-images' in f:\n",
        "            with gzip.open(f, 'rb') as data:\n",
        "                _ = int.from_bytes(data.read(4), 'big')\n",
        "                num_images = int.from_bytes(data.read(4), 'big')\n",
        "                rows = int.from_bytes(data.read(4), 'big')\n",
        "                cols = int.from_bytes(data.read(4), 'big')\n",
        "                train_images = data.read()\n",
        "                x_train = np.frombuffer(train_images, dtype=np.uint8)\n",
        "                x_train = x_train.reshape((num_images, rows, cols))\n",
        "        elif 'train-labels' in f:\n",
        "            with gzip.open(f, 'rb') as data:\n",
        "                train_labels = data.read()[8:]\n",
        "                y_train = np.frombuffer(train_labels, dtype=np.uint8)\n",
        "        if 't10k-images' in f:\n",
        "            with gzip.open(f, 'rb') as data:\n",
        "                _ = int.from_bytes(data.read(4), 'big')\n",
        "                num_images = int.from_bytes(data.read(4), 'big')\n",
        "                rows = int.from_bytes(data.read(4), 'big')\n",
        "                cols = int.from_bytes(data.read(4), 'big')\n",
        "                test_images = data.read()\n",
        "                x_test = np.frombuffer(test_images, dtype=np.uint8)\n",
        "                x_test = x_test.reshape((num_images, rows, cols))\n",
        "        elif 't10k-labels' in f:\n",
        "            with gzip.open(f, 'rb') as data:\n",
        "                test_labels = data.read()[8:]\n",
        "                y_test = np.frombuffer(test_labels, dtype=np.uint8)\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test   "
      ],
      "metadata": {
        "id": "N3y4h-HCZmHS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui passamos o caminho: MNIST path\n",
        "# Obviamente os Dados já foram baixados com antecedencia\n",
        "\n",
        "mnist_path = '/content/drive/MyDrive/2_DEEP_LEARNING_REDES_NEURAIS_Jorge/1_Pytorch_Deep_Learning/Pytorch_examples/Rede_neural_exemplo_Pytorch/data/MNIST/raw/mnist_raw'"
      ],
      "metadata": {
        "id": "9tSrFVE1Z3wU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui utilizamos a função \"get_images()\" \n",
        "\n",
        "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)"
      ],
      "metadata": {
        "id": "ujkiRCvxZ_qi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nesta celula não NORMALIZAMOS como fizímos no script anterior\n",
        "\n",
        "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)\n",
        "y_train = y_train_num[:50000].reshape(50000, 1)\n",
        "\n",
        "x_val = x_train_num[50000:].reshape(10000, -1).astype(np.float)\n",
        "y_val = y_train_num[50000:].reshape(10000, 1)\n",
        "\n",
        "x_test = x_test_num.copy().reshape(10000, -1).astype(np.float)\n",
        "y_test = y_test_num.copy().reshape(10000, 1)"
      ],
      "metadata": {
        "id": "7w_y57goaI6A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Os valores atuais da Média, Desvio padrão e o valor mínimo são, respectivamente:\")\n",
        "x_train.mean(), x_train.std(), x_train.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y3TjC7raWhg",
        "outputId": "4f9314a8-bc1f-4900-d07f-dfef140e28ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os valores atuais da Média, Desvio padrão e o valor mínimo são, respectivamente:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33.395157, 78.66619, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui NORMALIZAMOS nossos Dados\n",
        "\n",
        "def normalise(x_mean, x_std, x_data):\n",
        "    return (x_data - x_mean) / x_std\n",
        "    "
      ],
      "metadata": {
        "id": "e5o4HoO_aWe8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui definimos a MÉDIA e o DESVIO PADRÃO\n",
        "# Como estes objetos iremos NORMALIZAR nossos Dados de VALIDAÇÃO e Dados de TEST\n",
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "\n",
        "\n",
        "# A lógica é a seguinte: Normalizamos com a estatística \n",
        "x_train = normalise(x_mean, x_std, x_train)\n",
        "x_val = normalise(x_mean, x_std, x_val)\n",
        "x_test = normalise(x_mean, x_std, x_test)"
      ],
      "metadata": {
        "id": "FkwPt56laWcs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui podemos verificar que a Média e o Desvio Padrão em nossos Dados\n",
        "print(\"Média e desvio Padrão, respectivamente, em nossos Dados de TREINAMENTO\")\n",
        "print(x_train.mean(), x_train.std())\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Média e desvio Padrão, respectivamente, em nossos Dados de VALIDAÇÃO\")\n",
        "print(x_val.mean(), x_val.std())\n"
      ],
      "metadata": {
        "id": "2txzkhCQaWae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82454669-baf7-4ebb-b964-9f5b6646eaf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média e desvio Padrão, respectivamente, em nossos Dados de TREINAMENTO\n",
            "-3.1638146e-07 0.99999934\n",
            "\n",
            "Média e desvio Padrão, respectivamente, em nossos Dados de VALIDAÇÃO\n",
            "-0.0058509559841872305 0.9924333474151182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando nossos Dados de Imagens"
      ],
      "metadata": {
        "id": "sKYgpCd-80Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_number(image):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    "
      ],
      "metadata": {
        "id": "gatqeTjxaWTB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_idx = np.random.randint(len(y_test))\n",
        "print(f'La imagen muestreada representa un: {y_test[rnd_idx, 0]}')\n",
        "plot_number(x_test_num[rnd_idx])"
      ],
      "metadata": {
        "id": "dJbNJaiLaWQb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "216870ad-f409-4759-b55b-a96723ca439e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La imagen muestreada representa un: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir apresentamos as equações que usamos para construir nosso Modelo Neuronal:"
      ],
      "metadata": {
        "id": "yilxijZ6_I9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$z^1 = W^1 X  + b^1$$\n",
        "\n",
        "$$a^1 = ReLU(z^1)$$\n",
        "\n",
        "$$z^2 = W^2 a^1 + b^2$$\n",
        "\n",
        "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j e^{z_j}}$$\n",
        "\n",
        "$$\\mathcal{L}(\\hat{y}^i, y^i) = - y^i ln(\\hat{y}^i) = - ln(\\hat{y}^i)$$\n",
        "\n",
        "$$\\mathcal{J}(w, b) = \\frac{1}{num_samples} \\sum_{i=1}^{num_samples}- ln(\\hat{y}^i)$$"
      ],
      "metadata": {
        "id": "Y3p2tt67_Qzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções adicionais"
      ],
      "metadata": {
        "id": "RN33Pjrv_Vvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini-batches"
      ],
      "metadata": {
        "id": "D_XZpwGV_ZXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui construímos essa função que nos permitirá criar os `mini-batches` para fazer um treinamento em mini-batches. Isto consiste em enviar uma pequena seção de Dados (pequeno bloco de Dados) para fazer um treinamento mais eficiente, de que enviar um dado à vez."
      ],
      "metadata": {
        "id": "XYjCR114_iFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_minibatches(mb_size, x, y, shuffle = True):\n",
        "    '''\n",
        "    x  #amostras, 784\n",
        "    y  #amostras, 1\n",
        "    '''\n",
        "    assert x.shape[0] == y.shape[0], 'Erro na quantidade de amostras'\n",
        "    total_data = x.shape[0]\n",
        "    if shuffle: \n",
        "        idxs = np.arange(total_data)\n",
        "        np.random.shuffle(idxs)\n",
        "        x = x[idxs]\n",
        "        y = y[idxs]  \n",
        "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
      ],
      "metadata": {
        "id": "vIgfs2AC9O_Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nossa classe Linear, ReLU e Sequential"
      ],
      "metadata": {
        "id": "DwPheMoOBqUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Serão objetos que vão heredar atributos de NumPy (Usar com cuidado!!!)\n",
        "\n",
        "class np_tensor(np.ndarray): pass"
      ],
      "metadata": {
        "id": "wWZ1btG_Bl8S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe Linear"
      ],
      "metadata": {
        "id": "vTtOL-__DA6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        '''\n",
        "        Init parameters utilizando Kaiming He\n",
        "        '''\n",
        "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
        "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
        "    def __call__(self, X): # Este é o foward da classe linear\n",
        "        Z = self.W @ X + self.b\n",
        "        return Z\n",
        "    def backward(self, X, Z):\n",
        "        X.grad = self.W.T @ Z.grad\n",
        "        self.W.grad = Z.grad @ X.T\n",
        "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)    "
      ],
      "metadata": {
        "id": "n4PIPyvPC7qH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe ReLU"
      ],
      "metadata": {
        "id": "RnRxAVk8SmmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU():\n",
        "    def __call__(self, Z): # ReLU recebe z = wx + b\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def backward(self, Z, A): # A saída de ReLU é: A\n",
        "        Z.grad = A.grad.copy()\n",
        "        Z.grad[Z <= 0] = 0"
      ],
      "metadata": {
        "id": "UrcMVYvdSjw_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classe Sequential"
      ],
      "metadata": {
        "id": "Fe804ziyTsaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential_layers():\n",
        "    def __init__(self, layers):\n",
        "        '''\n",
        "        layers - lista que contiene objetos de tipo Linear, ReLU\n",
        "        '''\n",
        "        self.layers = layers\n",
        "        self.x = None\n",
        "        self.outputs = {}\n",
        "    def __call__(self, X):\n",
        "        self.x = X \n",
        "        self.outputs['l0'] = self.x\n",
        "        for i, layer in enumerate(self.layers, 1):\n",
        "            self.x = layer(self.x)\n",
        "            self.outputs['l'+str(i)]=self.x\n",
        "        return self.x\n",
        "    def backward(self):\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
        "    def update(self, learning_rate = 1e-3):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ReLU): continue\n",
        "            layer.W = layer.W - learning_rate * layer.W.grad\n",
        "            layer.b = layer.b - learning_rate * layer.b.grad\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.__call__(X))  "
      ],
      "metadata": {
        "id": "U9d5NYJ4TpiO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de Custo (ou Perda)"
      ],
      "metadata": {
        "id": "zsQROoc9WgD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmaxXEntropy(x, y):\n",
        "    batch_size = x.shape[1]\n",
        "    exp_scores = np.exp(x)\n",
        "    probs = exp_scores / exp_scores.sum(axis = 0)\n",
        "    preds = probs.copy()\n",
        "    # Costo\n",
        "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
        "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
        "    # Calcular gradientes\n",
        "    probs[y.squeeze(), np.arange(batch_size)] -= 1 #dl/dx\n",
        "    x.grad = probs.copy()\n",
        "    \n",
        "    return preds, cost"
      ],
      "metadata": {
        "id": "2jYQbrYpWfTT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop de Treinamento"
      ],
      "metadata": {
        "id": "2W9r_QAdWrrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, mb_size=128, learning_rate = 1e-3):\n",
        "    for epoch in range(epochs):\n",
        "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
        "            scores = model(x.T.view(np_tensor))\n",
        "            _, cost = softmaxXEntropy(scores, y)\n",
        "            model.backward()\n",
        "            model.update(learning_rate)\n",
        "        print(f'costo: {cost}, accuracy: {accuracy(x_val, y_val, mb_size)}')"
      ],
      "metadata": {
        "id": "nu2_TJN0WrJT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(x, y, mb_size):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (x, y) in enumerate(create_minibatches(mb_size, x, y)):\n",
        "        pred = model(x.T.view(np_tensor))\n",
        "        correct += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
        "        total += pred.shape[1]\n",
        "    return correct/total"
      ],
      "metadata": {
        "id": "YeBxsvaLXSRw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential_layers([Linear(784, 200), ReLU(), Linear(200, 200), ReLU(), Linear(200, 10)])\n",
        "\n",
        "mb_size = 512\n",
        "learning_rate = 1e-4\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "gXWEURJLXWGL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train(model, epochs, mb_size, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj-NsvJuYjv-",
        "outputId": "82060bfb-7e5c-4588-caf8-adb8fe0238bc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "costo: 0.35623432340012495, accuracy: 0.9165\n",
            "costo: 0.15954799884024456, accuracy: 0.9344\n",
            "costo: 0.20614744575150604, accuracy: 0.9448\n",
            "costo: 0.18676819283935187, accuracy: 0.9507\n",
            "costo: 0.1201501360896936, accuracy: 0.9539\n",
            "costo: 0.15738641879276632, accuracy: 0.9578\n",
            "costo: 0.12797190275422915, accuracy: 0.9623\n",
            "costo: 0.15459350799042978, accuracy: 0.9635\n",
            "costo: 0.1354549942400075, accuracy: 0.9645\n",
            "costo: 0.09681969854077661, accuracy: 0.9657\n",
            "costo: 0.0775099845601259, accuracy: 0.9678\n",
            "costo: 0.12870073816181785, accuracy: 0.9698\n",
            "costo: 0.08940924725230928, accuracy: 0.9682\n",
            "costo: 0.0914506183960762, accuracy: 0.969\n",
            "costo: 0.060300220902318194, accuracy: 0.9703\n",
            "costo: 0.08032673129340226, accuracy: 0.9702\n",
            "costo: 0.062409800210555234, accuracy: 0.9709\n",
            "costo: 0.07304728209435828, accuracy: 0.9716\n",
            "costo: 0.05096272143571268, accuracy: 0.9727\n",
            "costo: 0.06191695867769552, accuracy: 0.9735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(accuracy(x_test, y_test, mb_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSrtkTRnYm7i",
        "outputId": "0123ef5d-74a9-480a-d206-df4fc2213023"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "idx = np.random.randint(len(y_test))\n",
        "plot_number(x_test_num[idx])\n",
        "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
        "print(f'O valor predito é: {pred}, o valor real é:{y_test[idx][0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "qRwEhsXfYokT",
        "outputId": "35e29fc8-a442-4b46-a5dc-e79dfc11ea52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O valor predito é: 6, o valor real é:6\n"
          ]
        }
      ]
    }
  ]
}